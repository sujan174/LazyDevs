{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2dde9e89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sujanh/Documents/github/NewIdea/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/sujanh/Documents/github/NewIdea/.venv/lib/python3.10/site-packages/speechbrain/utils/torch_audio_backend.py:57: UserWarning: torchaudio._backend.list_audio_backends has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n",
      "  available_backends = torchaudio.list_audio_backends()\n",
      "/var/folders/7k/ksyl4fmn4pxgkv9rcm_wxglc0000gn/T/ipykernel_68164/2030016041.py:12: UserWarning: Module 'speechbrain.pretrained' was deprecated, redirecting to 'speechbrain.inference'. Please update your script. This is a change from SpeechBrain 1.0. See: https://github.com/speechbrain/speechbrain/releases/tag/v1.0.0\n",
      "  from speechbrain.pretrained import EncoderClassifier\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ§  Loading SpeechBrain speaker recognition model on 'cpu'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sujanh/Documents/github/NewIdea/.venv/lib/python3.10/site-packages/speechbrain/utils/autocast.py:188: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  wrapped_fwd = torch.cuda.amp.custom_fwd(fwd, cast_inputs=cast_inputs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… SpeechBrain model loaded.\n",
      "ðŸŽ¤ Starting AssemblyAI transcription for '/Users/sujanh/Downloads/data2/Segment 2.mp3'...\n",
      "âœ… Transcription complete. Extracting speaker audio clips...\n",
      "  -> Exporting merged audio for Speaker A...\n",
      "  -> Exporting merged audio for Speaker B...\n",
      "  -> Exporting merged audio for Speaker C...\n",
      "\n",
      "--- Starting Speaker Enrollment ---\n",
      "Creating voiceprint for 'spk1'...\n",
      "Creating voiceprint for 'spk2'...\n",
      "Creating voiceprint for 'spk3'...\n",
      "Creating voiceprint for 'spk4'...\n",
      "--- Enrollment Complete ---\n",
      "\n",
      "--- Identifying Unknown Speakers ---\n",
      "  - Matched Speaker B -> spk1 (Confidence: 0.71)\n",
      "  - Matched Speaker A -> spk3 (Confidence: 0.78)\n",
      "  - Matched Speaker C -> spk4 (Confidence: 0.53)\n",
      "\n",
      "--- Generating Final Named Transcript ---\n",
      "âœ… Final transcript saved to 'Segment 2_final_transcript.txt'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "import warnings\n",
    "from typing import Dict, List, Any\n",
    "import tempfile\n",
    "import argparse\n",
    "\n",
    "# --- Core AI Libraries ---\n",
    "import assemblyai as aai\n",
    "from pydub import AudioSegment\n",
    "from speechbrain.pretrained import EncoderClassifier\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Suppress user warnings for a cleaner output\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "\n",
    "class AssemblyAIHandler:\n",
    "    \"\"\"\n",
    "    Handles all interactions with the AssemblyAI API, including transcription,\n",
    "    diarization, and extraction of speaker audio clips into a temporary directory.\n",
    "    \"\"\"\n",
    "    def __init__(self, api_key: str):\n",
    "        if not api_key:\n",
    "            raise ValueError(\"AssemblyAI API key is required.\")\n",
    "        aai.settings.api_key = api_key\n",
    "        self.transcriber = aai.Transcriber()\n",
    "\n",
    "    def transcribe_and_extract(self, audio_path: str, temp_dir: str) -> (aai.Transcript, Dict[str, str]):\n",
    "        \"\"\"\n",
    "        Transcribes the audio, extracts each speaker's utterances into merged\n",
    "        audio files within a temporary directory.\n",
    "        \n",
    "        Returns:\n",
    "            - The full transcript object from AssemblyAI.\n",
    "            - A dictionary mapping generic speaker labels to their temp audio file paths.\n",
    "        \"\"\"\n",
    "        print(f\"ðŸŽ¤ Starting AssemblyAI transcription for '{audio_path}'...\")\n",
    "        config = aai.TranscriptionConfig(speaker_labels=True)\n",
    "        transcript = self.transcriber.transcribe(audio_path, config)\n",
    "\n",
    "        if transcript.status == aai.TranscriptStatus.error:\n",
    "            raise RuntimeError(f\"Transcription failed: {transcript.error}\")\n",
    "\n",
    "        if not transcript.utterances:\n",
    "            raise ValueError(\"Diarization failed. The audio might be too short or have only one speaker.\")\n",
    "\n",
    "        print(\"âœ… Transcription complete. Extracting speaker audio clips...\")\n",
    "        \n",
    "        try:\n",
    "            original_audio = AudioSegment.from_file(audio_path)\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Error loading audio with pydub: {e}. Ensure FFmpeg is installed.\")\n",
    "\n",
    "        speaker_segments = {}\n",
    "        for utterance in transcript.utterances:\n",
    "            speaker = utterance.speaker\n",
    "            clip = original_audio[utterance.start:utterance.end]\n",
    "            if speaker not in speaker_segments:\n",
    "                speaker_segments[speaker] = clip\n",
    "            else:\n",
    "                speaker_segments[speaker] += clip\n",
    "\n",
    "        unknown_speaker_paths = {}\n",
    "        for speaker, merged_audio in speaker_segments.items():\n",
    "            speaker_file_path = os.path.join(temp_dir, f\"SPEAKER_{speaker}.mp3\")\n",
    "            print(f\"  -> Exporting merged audio for Speaker {speaker}...\")\n",
    "            merged_audio.export(speaker_file_path, format=\"mp3\")\n",
    "            unknown_speaker_paths[speaker] = speaker_file_path\n",
    "            \n",
    "        return transcript, unknown_speaker_paths\n",
    "\n",
    "\n",
    "class SpeechBrainIdentifier:\n",
    "    \"\"\"\n",
    "    Handles voiceprint creation and speaker identification using SpeechBrain's\n",
    "    powerful ECAPA-TDNN model and the Hungarian algorithm for optimal assignment.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        print(f\"\\nðŸ§  Loading SpeechBrain speaker recognition model on '{self.device}'...\")\n",
    "        self.classifier = EncoderClassifier.from_hparams(\n",
    "            source=\"speechbrain/spkrec-ecapa-voxceleb\",\n",
    "            savedir=\"pretrained_models/spkrec-ecapa-voxceleb\",\n",
    "            run_opts={\"device\": self.device}\n",
    "        )\n",
    "        print(\"âœ… SpeechBrain model loaded.\")\n",
    "\n",
    "    def _create_voiceprint(self, audio_path: str) -> torch.Tensor:\n",
    "        \"\"\"Computes a speaker embedding from an audio file.\"\"\"\n",
    "        try:\n",
    "            signal, fs = torchaudio.load(audio_path)\n",
    "            if fs != 16000:\n",
    "                signal = torchaudio.transforms.Resample(orig_freq=fs, new_freq=16000)(signal)\n",
    "            if signal.shape[0] > 1:\n",
    "                signal = torch.mean(signal, dim=0, keepdim=True)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                embedding = self.classifier.encode_batch(signal)\n",
    "                return torch.nn.functional.normalize(embedding, p=2, dim=2).squeeze()\n",
    "        except Exception as e:\n",
    "            print(f\"  - WARNING: Could not create voiceprint for {audio_path}: {e}\")\n",
    "            return None\n",
    "\n",
    "    def enroll_speakers(self, speaker_samples: Dict[str, str]) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"Creates a voiceprint database from enrolled speaker samples.\"\"\"\n",
    "        print(\"\\n--- Starting Speaker Enrollment ---\")\n",
    "        voiceprint_db = {}\n",
    "        for name, path in speaker_samples.items():\n",
    "            print(f\"Creating voiceprint for '{name}'...\")\n",
    "            embedding = self._create_voiceprint(path)\n",
    "            if embedding is not None:\n",
    "                voiceprint_db[name] = embedding\n",
    "        print(\"--- Enrollment Complete ---\\n\")\n",
    "        return voiceprint_db\n",
    "\n",
    "    def identify_speakers(self, unknown_clips: Dict[str, str], enrolled_voiceprints: Dict[str, torch.Tensor]) -> Dict[str, str]:\n",
    "        \"\"\"\n",
    "        Finds the optimal one-to-one mapping of unknown speakers to enrolled speakers.\n",
    "        \"\"\"\n",
    "        print(\"--- Identifying Unknown Speakers ---\")\n",
    "        unknown_voiceprints = {}\n",
    "        for speaker_label, path in unknown_clips.items():\n",
    "            embedding = self._create_voiceprint(path)\n",
    "            if embedding is not None:\n",
    "                unknown_voiceprints[speaker_label] = embedding\n",
    "\n",
    "        enrolled_names = list(enrolled_voiceprints.keys())\n",
    "        unknown_labels = list(unknown_voiceprints.keys())\n",
    "        \n",
    "        if not unknown_labels:\n",
    "            print(\"No unknown speakers to identify.\")\n",
    "            return {}\n",
    "\n",
    "        # Create a similarity matrix (higher is better)\n",
    "        similarity_matrix = np.zeros((len(enrolled_names), len(unknown_labels)))\n",
    "        cosine_similarity = torch.nn.CosineSimilarity(dim=0)\n",
    "        for i, name in enumerate(enrolled_names):\n",
    "            for j, label in enumerate(unknown_labels):\n",
    "                score = cosine_similarity(enrolled_voiceprints[name], unknown_voiceprints[label]).item()\n",
    "                similarity_matrix[i, j] = score\n",
    "\n",
    "        # Use the Hungarian algorithm on a cost matrix to find the optimal assignment\n",
    "        row_ind, col_ind = linear_sum_assignment(1 - similarity_matrix)\n",
    "\n",
    "        speaker_map = {}\n",
    "        assigned_unknowns = set()\n",
    "        confidence_threshold = 0.50 # This can be tuned\n",
    "\n",
    "        for r, c in zip(row_ind, col_ind):\n",
    "            enrolled_name = enrolled_names[r]\n",
    "            unknown_label = unknown_labels[c]\n",
    "            score = similarity_matrix[r, c]\n",
    "\n",
    "            if score > confidence_threshold:\n",
    "                speaker_map[unknown_label] = enrolled_name\n",
    "                assigned_unknowns.add(unknown_label)\n",
    "                print(f\"  - Matched Speaker {unknown_label} -> {enrolled_name} (Confidence: {score:.2f})\")\n",
    "        \n",
    "        unknown_count = 1\n",
    "        for label in unknown_labels:\n",
    "            if label not in assigned_unknowns:\n",
    "                unknown_name = f\"Unknown Speaker {unknown_count}\"\n",
    "                speaker_map[label] = unknown_name\n",
    "                print(f\"  - Could not confidently match Speaker {label}. Assigning as {unknown_name}.\")\n",
    "                unknown_count += 1\n",
    "                \n",
    "        return speaker_map\n",
    "\n",
    "\n",
    "class TranscriptionPipeline:\n",
    "    \"\"\"The main orchestrator for the entire process.\"\"\"\n",
    "    def __init__(self, api_key: str):\n",
    "        self.assembly_handler = AssemblyAIHandler(api_key)\n",
    "        self.speechbrain_identifier = SpeechBrainIdentifier()\n",
    "\n",
    "    def run(self, main_audio_path: str, speaker_samples: Dict[str, str]):\n",
    "        # Use a temporary directory that cleans itself up automatically\n",
    "        with tempfile.TemporaryDirectory() as temp_dir:\n",
    "            # Step 1: Transcribe and extract audio clips\n",
    "            transcript, unknown_clips = self.assembly_handler.transcribe_and_extract(main_audio_path, temp_dir)\n",
    "\n",
    "            # Step 2: Enroll known speakers\n",
    "            enrolled_voiceprints = self.speechbrain_identifier.enroll_speakers(speaker_samples)\n",
    "\n",
    "            # Step 3: Identify the unknown speakers from the extracted clips\n",
    "            speaker_map = self.speechbrain_identifier.identify_speakers(unknown_clips, enrolled_voiceprints)\n",
    "\n",
    "            # Step 4: Generate and save the final, named transcript\n",
    "            self._generate_final_transcript(transcript, speaker_map, main_audio_path)\n",
    "\n",
    "    def _generate_final_transcript(self, transcript: aai.Transcript, speaker_map: Dict[str, str], audio_path: str):\n",
    "        print(\"\\n--- Generating Final Named Transcript ---\")\n",
    "        base_name, _ = os.path.splitext(os.path.basename(audio_path))\n",
    "        output_filename = f\"{base_name}_final_transcript.txt\"\n",
    "\n",
    "        with open(output_filename, \"w\", encoding=\"utf-8\") as f:\n",
    "            for utterance in transcript.utterances:\n",
    "                speaker_label = utterance.speaker\n",
    "                # Get the real name from our map, or keep the generic label if unmapped\n",
    "                final_name = speaker_map.get(speaker_label, f\"Unmapped Speaker {speaker_label}\")\n",
    "                line = f\"Speaker {final_name}: {utterance.text}\\n\"\n",
    "                f.write(line)\n",
    "        \n",
    "        print(f\"âœ… Final transcript saved to '{output_filename}'\")\n",
    "\n",
    "\n",
    "# --- Main Execution Block ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Load environment variables from .env file\n",
    "    load_dotenv()\n",
    "    ASSEMBLYAI_API_KEY = os.getenv(\"ASSEMBLYAI_API_KEY\")\n",
    "\n",
    "    if not ASSEMBLYAI_API_KEY:\n",
    "        print(\"!!! ERROR: ASSEMBLYAI_API_KEY not found. Please create a .env file. !!!\")\n",
    "    else:\n",
    "        # --- Define Your Inputs Here ---\n",
    "        \n",
    "        # 1. A dictionary mapping the desired speaker names to their sample audio file paths.\n",
    "        SPEAKER_SAMPLES = {\n",
    "            \"spk1\": \"/Users/sujanh/Downloads/data2/spk1.mp3\",\n",
    "            \"spk2\": \"/Users/sujanh/Downloads/data2/spk2.mp3\",\n",
    "            \"spk3\": \"/Users/sujanh/Downloads/data2/spk3.mp3\",\n",
    "            \"spk4\": \"/Users/sujanh/Downloads/data2/spk4.mp3\"\n",
    "        }\n",
    "\n",
    "        # 2. The path to the main meeting audio file you want to process.\n",
    "        MAIN_AUDIO_FILE = \"/Users/sujanh/Downloads/data2/Segment 2.mp3\"\n",
    "\n",
    "        # --- Execution ---\n",
    "        try:\n",
    "            pipeline = TranscriptionPipeline(api_key=ASSEMBLYAI_API_KEY)\n",
    "            pipeline.run(main_audio_path=MAIN_AUDIO_FILE, speaker_samples=SPEAKER_SAMPLES)\n",
    "        except (ValueError, RuntimeError) as e:\n",
    "            print(f\"\\n--- A critical error occurred ---\")\n",
    "            print(e)\n",
    "        except Exception as e:\n",
    "            print(f\"\\n--- An unexpected error occurred ---\")\n",
    "            print(e)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
